{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "Always make sure whatever environment you're working in has the necessary libraries and packages installed for your model to run. Here, I already pre-made a Docker container with all the necessary dependencies installed but in rl you'll have many alternatives, like virtual/conda environments, you can use for your specific project. What those things are is kinda out of the scope of this tutorial, so just take my word for granted lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** RUN THIS BEFORE EVERYTHING! \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "I looked around the Internet to find a suitable dataset for this project. Ended up using a Kaggle dataset for classifying gene expression for types of leukemia!\n",
    "\n",
    "Couple of options: \n",
    "1. Found a GEO entry here: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE121711 as I went through open source papers.\n",
    "The actual R script I wrote to get the file from the internet and process it is in the utils folder, but otherwise the actual preprocessing of the dataset is below. Personally, I couldn't get the accuracy past maybe 30% but feel free to try it out on your own!\n",
    "\n",
    "2. There's a random Breast cancer dataset I found on Kaggle - also put it in the processed direcotory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "This is the initial shape/dimensions of the label data:  (72, 2)\n",
      "======================================================================\n",
      "This is the initial shape/dimensions of the training data:  (7129, 78)\n",
      "======================================================================\n",
      "This is the initial shape/dimensions of the test data:  (7129, 70)\n",
      "(7129, 38) (38,) (7129, 34) (34,)\n"
     ]
    }
   ],
   "source": [
    "# FUNCTION FOR LOADING IN THE DATA INTO NUMPY ARRAY: DO NOT TOUCH\n",
    "def load_data(name, filepath): \n",
    "    \n",
    "    # load in the CSV data into a pandas dataframe and briefly explore what it looks like \n",
    "    data = pd.read_csv(filepath, header=0)\n",
    "    print(\"======================================================================\")\n",
    "    print(f\"This is the initial shape/dimensions of the {name} data: \", data.shape)\n",
    "\n",
    "    return data \n",
    "\n",
    "labels = load_data('label', '../data/kaggle/actual.csv')\n",
    "train_data = load_data('training', '../data/kaggle/train.csv')\n",
    "test_data = load_data('test', '../data/kaggle/test.csv')\n",
    "\n",
    "def process_data(labels, train_data, test_data): \n",
    "\n",
    "    # remove superfluous columns from the train_data, test_data (call)\n",
    "    train_data, test_data = train_data.loc[:, [col for col in train_data.columns if \"call\" not in col]], test_data.loc[:, [col for col in test_data.columns if \"call\" not in col]]\n",
    "\n",
    "    # next remove the gene description, gene accession number, not absolutely necessary here \n",
    "    train_data, test_data = train_data.iloc[:,2:], test_data.iloc[:,2:]\n",
    "\n",
    "    # next, binarize the labels\n",
    "    binarized_labels = {\"ALL\": 0, \"AML\":1}\n",
    "    labels[\"cancer\"] = labels[\"cancer\"].map(binarized_labels)\n",
    "    # print(labels.tail(10))\n",
    "\n",
    "    # next, set the train and test labels individually \n",
    "    train_labels = np.array([labels.iloc[int(patient)-1, 1] for patient in train_data.columns])\n",
    "    test_labels = np.array([labels.iloc[int(patient)-1, 1] for patient in test_data.columns])\n",
    "\n",
    "    # now extract the features by converting dfs to numpy arrays \n",
    "    train_features, test_features = np.array(train_data), np.array(test_data)\n",
    "\n",
    "    # print out dimensions for checking\n",
    "    print(train_features.shape, train_labels.shape, test_features.shape, test_labels.shape)\n",
    "\n",
    "    return train_features, train_labels, test_features, test_labels\n",
    "\n",
    "train_features, train_labels, test_features, test_labels = process_data(labels, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY MIN-MAX SCALING AND VIZ FEATURE DISTRIBUTION\n",
    "\n",
    "def min_max_scale(features): \n",
    "    minimum, maximum = np.min(features), np.max(features)\n",
    "    return (features - minimum) / (maximum - minimum) \n",
    "\n",
    "# train_features, testing_features = min_max_scale(train_features), min_max_scale(test_features)\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "# test_features = scaler.transform(test_features)\n",
    "\n",
    "def plot_feature_distributions(idx, title, dist_to_plot, n_bins): \n",
    "    fig, ax = plt.subplots(1, 2, tight_layout=True)\n",
    "    ax[idx].hist(dist_to_plot, bins=n_bins)\n",
    "    ax[idx].title(title)\n",
    "    \n",
    "# plot_feature_distributions(1, \"Scaled Data\", flattened, len(flattened))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since the train and test data were already set for us, there's no need to set the splits. However for other datasets we're going to need to set the training and testing splits for the data after having loaded it into Numpy array format, so use this section as you will!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_splits(data, splits:dict): \n",
    "\n",
    "    # randomly shuffle the data before splitting it \n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # now split it \n",
    "    num_rows, num_cols = data.shape \n",
    "    training_features, training_labels = data[:int(splits['training']*num_rows),1:].T, data[:int(splits['training']*num_rows),0].T\n",
    "    print(training_labels)\n",
    "    testing_features, testing_labels = data[:int(splits['testing']*num_rows),1:].T,  data[:int(splits['testing']*num_rows),0].T\n",
    "    print(f'Shape of training features is {training_features.shape} and labels is {training_labels.shape}')\n",
    "    print(f'Shape of testing features is {testing_features.shape} and labels is {testing_labels.shape}')\n",
    "    return training_features, training_labels, testing_features, testing_labels \n",
    "\n",
    "# we're going to allocate 80% of the data for training, 20% for testing. The testing entry is technically not used but it's there for clarity \n",
    "# training_features, training_labels, testing_features, testing_labels = set_splits(data, {'training': 0.8, 'testing': 0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction \n",
    "As a recap, these are the dimensions of the model that we will use (training data) for any m training examples (here, it's 24 for training and 6 for testing). For i=1...m training examples, just add an extra dimension of the training examples to the matrices and multiply with that as a common internal dimension:\n",
    " \n",
    "- X = (1000, m) \n",
    "- W1 = (1000, 500), B1 = (500, 1) {bias can be 1 dimensional here b/c of broadcasting!}, Z1 = (500, m), A1 = (500, m)\n",
    "- W2 = (500, 100), B1 = (100, 1), Z2 = (100, m), A2 = (100, m)\n",
    "- W3 = (1, 100), B1 = (1, 1), Z3 = (1, m), A3 = (1, m) => last layer, to which we apply sigmoid function!\n",
    "\n",
    "Of note is that we only are using 2 layers in this network, which I believe is the practical amount for most tasks using linear neural networks (generally there's diminishing returns past 2-3 layers). Feel free to experiment and add more to this model though! \n",
    "\n",
    "Also, note that we don't define the loss function explicitly in the below model. This is because the choice of loss function is implicit in the formulas for the derivatives noted (we calculated the derivatives by hand earlier), but it's fairly straightforward to just use built-in libraries to compute the derivative. Thus the other parameters are pre-set as follows: \n",
    "\n",
    "- Optimizer: Vanilla Gradient Descent \n",
    "- Number epochs: 25 (change this as much as desired!)\n",
    "- Learning rate: 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_c/xlv9fsg15_df99yllrpzxrw80000gn/T/ipykernel_60721/3882566409.py:19: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-Z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 0 | Accuracy: 0.7105263157894737\n",
      "Epoch number: 5 | Accuracy: 0.7105263157894737\n",
      "Epoch number: 10 | Accuracy: 0.7105263157894737\n",
      "Epoch number: 15 | Accuracy: 0.7105263157894737\n",
      "Epoch number: 20 | Accuracy: 0.3684210526315789\n",
      "Epoch number: 25 | Accuracy: 0.8421052631578947\n",
      "Epoch number: 30 | Accuracy: 0.6052631578947368\n",
      "Epoch number: 35 | Accuracy: 0.631578947368421\n",
      "Epoch number: 40 | Accuracy: 0.7894736842105263\n",
      "Epoch number: 45 | Accuracy: 0.8421052631578947\n",
      "Epoch number: 50 | Accuracy: 0.9473684210526315\n",
      "Epoch number: 55 | Accuracy: 0.9473684210526315\n",
      "Epoch number: 60 | Accuracy: 0.9473684210526315\n",
      "Epoch number: 65 | Accuracy: 0.9473684210526315\n",
      "Epoch number: 70 | Accuracy: 0.9473684210526315\n",
      "Epoch number: 75 | Accuracy: 0.9473684210526315\n",
      "Epoch number: 80 | Accuracy: 0.9473684210526315\n",
      "Epoch number: 85 | Accuracy: 0.9473684210526315\n",
      "Epoch number: 90 | Accuracy: 0.9473684210526315\n",
      "Epoch number: 95 | Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If this were a multi-class classification problem, we would need something like this function. Just use this for ur own info as you will! \n",
    "def one_hot_encode_labels(Y, number_of_classes): \n",
    "    # --------------------------------------------------------------------------------------------------------------\n",
    "    # for every training example we're going to have a corresponding label\n",
    "    # however because the softmax function wants to compute a PROBABILITY, we should one-hot-encode the labels to be from range 0-1 to better compute the loss\n",
    "    # --------------------------------------------------------------------------------------------------------------\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max()+1))\n",
    "    one_hot_Y[np.arange(Y.size), Y.astype(int)] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def relu_activation(Z): \n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z): \n",
    "    return Z < 0\n",
    "\n",
    "def sigmoid_output(Z): \n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def softmax_output(Z):\n",
    "    return np.exp(Z) / sum(np.exp(Z))\n",
    "\n",
    "def initialize_parameters(input_layer_units, layer_1_units, layer_2_units):\n",
    "    \n",
    "    # we need to initialize our weight and bias matrices here. subtract 0.5 for numerical stability \n",
    "    W1 = np.random.randn(layer_1_units, input_layer_units) - 0.5\n",
    "    B1 = np.random.randn(layer_1_units, 1) \n",
    "    W2 = np.random.randn(layer_2_units, layer_1_units) - 0.5\n",
    "    B2 = np.random.randn(layer_2_units, 1)\n",
    "    \n",
    "    return W1, B1, W2, B2\n",
    "\n",
    "def forward_propagation(X, Y, W1, B1, W2, B2): \n",
    "\n",
    "    # print(\"------------------------------------\")\n",
    "    # Y = one_hot_encode_labels(Y, 6)\n",
    "    Z1 = np.matmul(W1, X) + B1\n",
    "    A1 = relu_activation(Z1)\n",
    "    # print(\"First activation's shape: \", A1.shape)\n",
    "    Z2 = np.matmul(W2, A1) + B2\n",
    "    A2 = sigmoid_output(Z2)\n",
    "    # print(\"Second activation's shape: \", A2.shape)\n",
    "    # print(\"------------------------------------\")\n",
    "\n",
    "    return Y, Z1, A1, Z2, A2\n",
    "\n",
    "def backward_propagation(num_examples, W2, X, Y, Z1, A1, A2): \n",
    "\n",
    "    dZ2 = A2 - Y\n",
    "    # print(\"dZ2 shape: \", dZ2.shape)\n",
    "    dW2 = (1 / num_examples) * np.matmul(dZ2, A1.T)\n",
    "    # print(\"dW2 shape: \", dW2.shape)\n",
    "    dB2 = (1 / num_examples) * np.sum(dZ2, axis=1, keepdims=True) \n",
    "    # print(\"dB2 shape: \", dB2.shape)\n",
    "    dZ1 = relu_derivative(Z1) * np.matmul(W2.T, dZ2)\n",
    "    # print(\"dZ1 shape: \", dZ1.shape)\n",
    "    dW1 = (1 / num_examples) * np.matmul(dZ1, X.T)\n",
    "    # print(\"dW1 shape: \", dW1.shape)\n",
    "    dB1 = (1 / num_examples) * np.sum(dZ1, axis=1, keepdims=True) \n",
    "    # print(\"dB1 shape: \", dB1.shape)\n",
    "\n",
    "    return dW2, dB2, dW1, dB1\n",
    "\n",
    "def pred(A):\n",
    "    # return np.array(A > 0.5, dtype='int')\n",
    "    return np.argmax(A, 0)\n",
    "\n",
    "def accuracy(pred, Y):\n",
    "    accuracy = 0\n",
    "    for i in range(pred.shape[1]):\n",
    "        if (pred[0][i] > 0.5 and Y[i] == 1) or (pred[0][i] <= 0.5 and Y[i] == 0):\n",
    "            accuracy += 1\n",
    "    return accuracy / len(Y)\n",
    "\n",
    "def gradient_descent(num_epochs, learning_rate, X, Y, W1, B1, W2, B2): \n",
    "     \n",
    "    # train for the decided number of epochs (basically # of passes over the same training dataset)\n",
    "    for epoch in range(num_epochs): \n",
    "\n",
    "        # make forward & backward pass to calculate loss\n",
    "        Y, Z1, A1, Z2, A2 = forward_propagation(X, Y, W1, B1, W2, B2)\n",
    "        dW2, dB2, dW1, dB1 = backward_propagation(34, W2, X, Y, Z1, A1, A2)\n",
    "        \n",
    "        # step - update params w/ gradient descent! \n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        W2 = W2 - learning_rate * dW2 \n",
    "        B1 = B1 - learning_rate * dB1\n",
    "        B2 = B2 - learning_rate * dB2\n",
    "\n",
    "        # calculate accuracy metrics! \n",
    "        if epoch % 5 == 0: \n",
    "            print(f'Epoch number: {epoch} | Accuracy: {accuracy(A2, Y)}')\n",
    " \n",
    "def train_model(X, Y): \n",
    "\n",
    "    # initialize parameters \n",
    "    num_epochs, learning_rate = 100, 0.001\n",
    "    W1, B1, W2, B2 = initialize_parameters(7129, 5500, 1)\n",
    "    gradient_descent(num_epochs, learning_rate, X, Y, W1, B1, W2, B2)\n",
    "\n",
    "train_model(train_features, train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
