{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "Always make sure whatever environment you're working in has the necessary libraries and packages installed for your model to run. \n",
    "\n",
    "This is the part that might be the most buggy as I'm not sure 100% what system everyone is working with, so please note that if you do run into bugs after trying to run the following section, speak up! What follows are instructions to set up a virtual environment in your cloned repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Run these before doing anything!  \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "I looked around the Internet to find a suitable dataset for this project. Ended up using a Kaggle dataset for types of leukemia from patients' gene expression data!\n",
    "\n",
    "Couple of other options for exploration: \n",
    "1. Found a GEO entry here for gene expression data linked to bladder cancer: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE121711 as I went through open source papers.\n",
    "The actual R script I wrote to get the file from the internet and process it is in the utils folder (unfortunately I failed to make a good model for this data), so feel free to use this code as a template to model this data too. \n",
    "\n",
    "2. Check out the breast cancer datasets on Kaggle - lots of options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "This is the initial shape/dimensions of the label data:  (72, 2)\n",
      "======================================================================\n",
      "This is the initial shape/dimensions of the training data:  (7129, 78)\n",
      "======================================================================\n",
      "This is the initial shape/dimensions of the test data:  (7129, 70)\n",
      "Shape of train features:  (7129, 38)\n",
      "Shape of trian labels:  (38,)\n",
      "Shape of test features:  (7129, 34)\n",
      "Shape of test labels:  (34,)\n"
     ]
    }
   ],
   "source": [
    "# loads in the data \n",
    "def load_data(name, filepath): \n",
    "    \n",
    "    # load in the CSV data into a pandas dataframe and briefly explore what it looks like \n",
    "    data = pd.read_csv(filepath, header=0)\n",
    "    print(\"======================================================================\")\n",
    "    print(f\"This is the initial shape/dimensions of the {name} data: \", data.shape)\n",
    "\n",
    "    return data \n",
    "\n",
    "labels = load_data('label', '../data/kaggle/actual.csv')\n",
    "train_data = load_data('training', '../data/kaggle/train.csv')\n",
    "test_data = load_data('test', '../data/kaggle/test.csv')\n",
    "\n",
    "# processes the data \n",
    "def process_data(labels, train_data, test_data): \n",
    "\n",
    "    # remove superfluous columns from the train_data, test_data (call)\n",
    "    train_data, test_data = train_data.loc[:, [col for col in train_data.columns if \"call\" not in col]], test_data.loc[:, [col for col in test_data.columns if \"call\" not in col]]\n",
    "\n",
    "    # next remove the gene description, gene accession number, not absolutely necessary here \n",
    "    train_data, test_data = train_data.iloc[:,2:], test_data.iloc[:,2:]\n",
    "\n",
    "    # next, binarize the labels\n",
    "    binarized_labels = {\"ALL\": 0, \"AML\":1}\n",
    "    labels[\"cancer\"] = labels[\"cancer\"].map(binarized_labels)\n",
    "\n",
    "    # next, set the train and test labels individually \n",
    "    train_labels = np.array([labels.iloc[int(patient)-1, 1] for patient in train_data.columns])\n",
    "    test_labels = np.array([labels.iloc[int(patient)-1, 1] for patient in test_data.columns])\n",
    "\n",
    "    # now extract the features by converting dfs to numpy arrays \n",
    "    train_features, test_features = np.array(train_data), np.array(test_data)\n",
    "\n",
    "    # print out dimensions for checking\n",
    "    print(\"Shape of train features: \", train_features.shape)\n",
    "    print(\"Shape of train labels: \", train_labels.shape)\n",
    "    print(\"Shape of test features: \", test_features.shape)\n",
    "    print(\"Shape of test labels: \", test_labels.shape)\n",
    "\n",
    "    return train_features, train_labels, test_features, test_labels\n",
    "\n",
    "train_features, train_labels, test_features, test_labels = process_data(labels, train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section preprocesses the input features by applying standard scaling from scikit-learn. Other options are min-max-scaling, log normalization, etc. all of which have their merits in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OPTIONAL: apply min-max scaling, see how the accuracy changes! \n",
    "def min_max_scale(features): \n",
    "    minimum, maximum = np.min(features), np.max(features)\n",
    "    return (features - minimum) / (maximum - minimum) \n",
    "\n",
    "# train_features, testing_features = min_max_scale(train_features), min_max_scale(test_features)\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "test_features = scaler.fit_transform(test_features) # <- technically this isn't the right tranform, should be fitted to the train scaling too, but it works somewhat\n",
    "\n",
    "def plot_feature_distributions(idx, title, dist_to_plot, n_bins): \n",
    "    fig, ax = plt.subplots(1, 2, tight_layout=True)\n",
    "    ax[idx].hist(dist_to_plot, bins=n_bins)\n",
    "    ax[idx].title(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since the train and test data were already set for us, there's no need to set the splits. However for other datasets we're going to need to set the training and testing splits for the data after having loaded it into Numpy array format, so use this section as you will!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_splits(data, splits:dict): \n",
    "\n",
    "    # randomly shuffle the data before splitting it, set a seed to ensure consistency\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # now split it \n",
    "    num_rows, num_cols = data.shape \n",
    "    training_features, training_labels = data[:int(splits['training']*num_rows),1:].T, data[:int(splits['training']*num_rows),0].T\n",
    "    print(training_labels)\n",
    "    testing_features, testing_labels = data[:int(splits['testing']*num_rows),1:].T,  data[:int(splits['testing']*num_rows),0].T\n",
    "    print(f'Shape of training features is {training_features.shape} and labels is {training_labels.shape}')\n",
    "    print(f'Shape of testing features is {testing_features.shape} and labels is {testing_labels.shape}')\n",
    "    return training_features, training_labels, testing_features, testing_labels \n",
    "\n",
    "# training_features, training_labels, testing_features, testing_labels = set_splits(data, {'training': 0.8, 'testing': 0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction \n",
    "As a recap, these are the dimensions of the model that we will use (training data) for any m training examples (here, it's 24 for training and 6 for testing). For i=1...m training examples, just add an extra dimension of the training examples to the matrices and multiply with that as a common internal dimension:\n",
    " \n",
    "- X = (1000, m) \n",
    "- W1 = (1000, 500), B1 = (500, 1) {bias can be 1 dimensional here b/c of broadcasting!}, Z1 = (500, m), A1 = (500, m)\n",
    "- W2 = (500, 100), B1 = (100, 1), Z2 = (100, m), A2 = (100, m)\n",
    "- W3 = (1, 100), B1 = (1, 1), Z3 = (1, m), A3 = (1, m) => last layer, to which we apply sigmoid function!\n",
    "\n",
    "Of note is that we only are using 2 layers in this network, which I believe is the practical amount for most tasks using linear neural networks (generally there's diminishing returns past 2-3 layers). Feel free to experiment and add more to this model though! \n",
    "\n",
    "Also, note that we don't define the loss function explicitly in the below model. This is because the choice of loss function is implicit in the formulas for the derivatives noted (we calculated the derivatives by hand earlier), but it's fairly straightforward to just use built-in libraries to compute the derivative. Thus the other parameters are pre-set as follows: \n",
    "\n",
    "- Optimizer: Vanilla Gradient Descent \n",
    "- Number epochs: 25 (change this as much as desired!)\n",
    "- Learning rate: 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_c/xlv9fsg15_df99yllrpzxrw80000gn/T/ipykernel_62431/1247277161.py:19: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-Z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 0 | Training Accuracy: 0.7105263157894737\n",
      "Epoch number: 10 | Training Accuracy: 0.7105263157894737\n",
      "Epoch number: 20 | Training Accuracy: 0.39473684210526316\n",
      "Epoch number: 30 | Training Accuracy: 0.7105263157894737\n",
      "Epoch number: 40 | Training Accuracy: 0.7105263157894737\n",
      "Epoch number: 50 | Training Accuracy: 0.8421052631578947\n",
      "Epoch number: 60 | Training Accuracy: 0.8421052631578947\n",
      "Epoch number: 70 | Training Accuracy: 0.8421052631578947\n",
      "Epoch number: 80 | Training Accuracy: 0.8157894736842105\n",
      "Epoch number: 90 | Training Accuracy: 0.9736842105263158\n",
      "Epoch number: 0 | Testing Accuracy: 0.7647058823529411\n",
      "Epoch number: 10 | Testing Accuracy: 0.7647058823529411\n",
      "Epoch number: 20 | Testing Accuracy: 0.7647058823529411\n",
      "Epoch number: 30 | Testing Accuracy: 0.7647058823529411\n",
      "Epoch number: 40 | Testing Accuracy: 0.7647058823529411\n",
      "Epoch number: 50 | Testing Accuracy: 0.7647058823529411\n",
      "Epoch number: 60 | Testing Accuracy: 0.7647058823529411\n",
      "Epoch number: 70 | Testing Accuracy: 0.7647058823529411\n",
      "Epoch number: 80 | Testing Accuracy: 0.7647058823529411\n",
      "Epoch number: 90 | Testing Accuracy: 0.7647058823529411\n",
      "Epoch number: 100 | Testing Accuracy: 0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If this were a multi-class classification problem, we would need something like this function. Just use this for ur own info as you will! \n",
    "def one_hot_encode_labels(Y, number_of_classes): \n",
    "    # --------------------------------------------------------------------------------------------------------------\n",
    "    # for every training example we're going to have a corresponding label\n",
    "    # however because the softmax function wants to compute a PROBABILITY, we should one-hot-encode the labels to be from range 0-1 to better compute the loss\n",
    "    # --------------------------------------------------------------------------------------------------------------\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max()+1))\n",
    "    one_hot_Y[np.arange(Y.size), Y.astype(int)] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def relu_activation(Z): \n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z): \n",
    "    return Z < 0\n",
    "\n",
    "def sigmoid_output(Z): \n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def initialize_parameters(input_layer_units, layer_1_units, layer_2_units):\n",
    "    \n",
    "    # we need to initialize our weight and bias matrices here. subtract 0.5 for numerical stability \n",
    "    W1 = np.random.randn(layer_1_units, input_layer_units) - 0.5\n",
    "    B1 = np.random.randn(layer_1_units, 1) \n",
    "    W2 = np.random.randn(layer_2_units, layer_1_units) - 0.5\n",
    "    B2 = np.random.randn(layer_2_units, 1)\n",
    "    \n",
    "    return W1, B1, W2, B2\n",
    "\n",
    "def forward_propagation(X, Y, W1, B1, W2, B2): \n",
    "\n",
    "    # Y = one_hot_encode_labels(Y, 6)\n",
    "    Z1 = np.matmul(W1, X) + B1\n",
    "    A1 = relu_activation(Z1)\n",
    "    # print(\"First activation's shape: \", A1.shape)\n",
    "    Z2 = np.matmul(W2, A1) + B2\n",
    "    A2 = sigmoid_output(Z2)\n",
    "    # print(\"Second activation's shape: \", A2.shape)\n",
    "\n",
    "    return Y, Z1, A1, Z2, A2\n",
    "\n",
    "def backward_propagation(num_examples, W2, X, Y, Z1, A1, A2): \n",
    "\n",
    "    dZ2 = A2 - Y\n",
    "    # print(\"dZ2 shape: \", dZ2.shape)\n",
    "    dW2 = (1 / num_examples) * np.matmul(dZ2, A1.T)\n",
    "    # print(\"dW2 shape: \", dW2.shape)\n",
    "    dB2 = (1 / num_examples) * np.sum(dZ2, axis=1, keepdims=True) \n",
    "    # print(\"dB2 shape: \", dB2.shape)\n",
    "    dZ1 = relu_derivative(Z1) * np.matmul(W2.T, dZ2)\n",
    "    # print(\"dZ1 shape: \", dZ1.shape)\n",
    "    dW1 = (1 / num_examples) * np.matmul(dZ1, X.T)\n",
    "    # print(\"dW1 shape: \", dW1.shape)\n",
    "    dB1 = (1 / num_examples) * np.sum(dZ1, axis=1, keepdims=True) \n",
    "    # print(\"dB1 shape: \", dB1.shape)\n",
    "\n",
    "    return dW2, dB2, dW1, dB1\n",
    "\n",
    "def accuracy(pred, Y):\n",
    "    accuracy = 0\n",
    "    for i in range(pred.shape[1]):\n",
    "        if (pred[0][i] > 0.5 and Y[i] == 1) or (pred[0][i] <= 0.5 and Y[i] == 0):\n",
    "            accuracy += 1\n",
    "    return accuracy / len(Y)\n",
    "\n",
    "def gradient_descent(num_epochs, learning_rate, X, Y, W1, B1, W2, B2): \n",
    "     \n",
    "    # train for the decided number of epochs (basically # of passes over the same training dataset)\n",
    "    for epoch in range(num_epochs): \n",
    "\n",
    "        # make forward & backward pass to calculate loss\n",
    "        Y, Z1, A1, Z2, A2 = forward_propagation(X, Y, W1, B1, W2, B2)\n",
    "        dW2, dB2, dW1, dB1 = backward_propagation(34, W2, X, Y, Z1, A1, A2)\n",
    "        \n",
    "        # step - update params w/ gradient descent! \n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        W2 = W2 - learning_rate * dW2 \n",
    "        B1 = B1 - learning_rate * dB1\n",
    "        B2 = B2 - learning_rate * dB2\n",
    "\n",
    "        # calculate accuracy metrics! \n",
    "        if epoch % 10 == 0: \n",
    "            print(f'Epoch number: {epoch} | Training Accuracy: {accuracy(A2, Y)}')\n",
    "    \n",
    "    return W1, W2, B1, B2\n",
    "        \n",
    "            \n",
    "def train_model(X, Y, num_epochs, learning_rate): \n",
    "\n",
    "    # initialize parameters & update them \n",
    "    W1, B1, W2, B2 = initialize_parameters(7129, 6000, 1)\n",
    "    W1, W2, B1, B2 = gradient_descent(num_epochs, learning_rate, X, Y, W1, B1, W2, B2)\n",
    "    \n",
    "    return W1, W2, B1, B2\n",
    "\n",
    "def test_model(X, Y,  W1, W2, B1, B2, num_epochs): \n",
    "    \n",
    "    # test on testing dataset! note you're not doing any updates here, you're just evaluating, so it goes much quicker than training does \n",
    "    for epoch in range(num_epochs+1): \n",
    "\n",
    "        # params from W1, B1, W2 ... so on are saved from earlier in computer memory! \n",
    "        Y, Z1, A1, Z2, A2 = forward_propagation(X, Y, W1, B1, W2, B2)\n",
    "\n",
    "        if epoch % 10 == 0: \n",
    "            print(f'Epoch number: {epoch} | Testing Accuracy: {accuracy(A2, Y)}')\n",
    "               \n",
    "\n",
    "num_epochs, learning_rate = 100, 0.001\n",
    "W1, W2, B1, B2 = train_model(train_features, train_labels, num_epochs, learning_rate)\n",
    "test_model(test_features, test_labels, W1, W2, B1, B2, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
